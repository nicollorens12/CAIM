{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 - Programming with Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install elasticsearch\n",
    "#%pip install elasticsearch-dsl\n",
    "#%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Modifying ElasticSearch index behavior\n",
    "\n",
    "In the previous session we had to clean manually the list of words in order to compute Zipf's and Heaps' laws. \n",
    "\n",
    "ElasticSearch allows using a pipeline of processes that allows to clean the text that is indexed discarding anything not useful.\n",
    "\n",
    "We are going to work with three of the usual processes:\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Token filtering (stopwords and stemming)\n",
    "\n",
    "The next cells allow configuring the default tokenizer for an index and analyze an example text. We are going to play a little bit with the possibilities and see what tokens result from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Index, analyzer, tokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Elasticsearch( hosts=['http://localhost:9200'], request_timeout=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Whitespace filter lowercase\n",
    "The whitespace tokenizer divides text into terms whenever it encounters any whitespace character.\n",
    "The lowercase tokenizer filter changes the token to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:303: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  return self._get_connection(using).indices.close(index=self._name, **kwargs)\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:303: ElasticsearchWarning: the default value for the ?wait_for_active_shards parameter will change from '0' to 'index-setting' in version 8; specify '?wait_for_active_shards=index-setting' to adopt the future default behaviour, or '?wait_for_active_shards=0' to preserve today's behaviour\n",
      "  return self._get_connection(using).indices.close(index=self._name, **kwargs)\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:324: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  self._get_connection(using).indices.exists(index=self._name, **kwargs)\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:421: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  return self._get_connection(using).indices.get_settings(\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:175: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  state = self._get_connection(using).cluster.state(\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:434: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  return self._get_connection(using).indices.put_settings(\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:292: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  return self._get_connection(using).indices.open(index=self._name, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('whitespace'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': '4ís', 'start_offset': 10, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was%', 'start_offset': 14, 'end_offset': 18, 'type': 'word', 'position': 3}\n",
      "{'token': '&printing', 'start_offset': 19, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the.', 'start_offset': 42, 'end_offset': 46, 'type': 'word', 'position': 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:248: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  return self._get_connection(using).indices.analyze(index=self._name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Standard\n",
    "The standard tokenizer divides text into terms on word boundaries, as defined by the Unicode Text Segmentation algorithm. It removes most punctuation symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('standard'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': '<ALPHANUM>', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': '<ALPHANUM>', 'position': 1}\n",
      "{'token': '4ís', 'start_offset': 10, 'end_offset': 13, 'type': '<ALPHANUM>', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': '<ALPHANUM>', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': '<ALPHANUM>', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': '<ALPHANUM>', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': '<ALPHANUM>', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': '<ALPHANUM>', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Letter\n",
    "The letter tokenizer divides text into terms whenever it encounters a character which is not a letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'ís', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter asciifolding\n",
    "Filter parameter can be a combination of filters. We use lowercase, but we also introduce asciifolding, this converts alphabetic, numeric, and symbolic characters that are not in the Basic Latin Unicode block (first 127 ASCII characters) to their ASCII equivalent, if one exists. For example, the filter changes à to a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'is', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter asciifolding + stop\n",
    "\n",
    "Using the last two filters, we will now be adding stop, this removes stop words from a token stream.\n",
    "\n",
    "When not customized, the filter removes the following English stop words by default:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding', 'stop']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter asciifolding + stop + snowball\n",
    "\n",
    "Snowball is a filter that stems words using a Snowball-generated stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding','stop', 'snowball']\n",
    ")\n",
    "   \n",
    "ind = Index('news', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'print', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'print', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **follow the instructions** of the documentation, index the documents from the previous session using the script 'IndexFilesPreprocess.py' and use the script 'CountWords.py' from the previous session to see how the set of tokens change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 2 The index reloaded\n",
    "\n",
    "We will now be using the `novels` index with diferent kinds of tokenizer and filters. For analizing results, we've modified `CountWords.py` as a class to import it's functionality to the notebook. We added diferent tokenizers and filters that in the previous section we've seen how the affect the text. Now we want to see how they affect the total word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CountWords import ElasticFunctionals\n",
    "\n",
    "results_df = pd.DataFrame(columns=['tokenizer', 'filters', 'word_count'])\n",
    "\n",
    "index = 'novels'\n",
    "\n",
    "ef = ElasticFunctionals(client)\n",
    "\n",
    "tokenizers = ['whitespace', 'standard', 'classic', 'letter']\n",
    "filters = [['lowercase'], ['lowercase','asciifolding'], ['lowercase', 'porter_stem'], ['lowercase', 'kstem'], ['lowercase', 'snowball']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:303: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  return self._get_connection(using).indices.close(index=self._name, **kwargs)\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:303: ElasticsearchWarning: the default value for the ?wait_for_active_shards parameter will change from '0' to 'index-setting' in version 8; specify '?wait_for_active_shards=index-setting' to adopt the future default behaviour, or '?wait_for_active_shards=0' to preserve today's behaviour\n",
      "  return self._get_connection(using).indices.close(index=self._name, **kwargs)\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:324: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  self._get_connection(using).indices.exists(index=self._name, **kwargs)\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:421: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  return self._get_connection(using).indices.get_settings(\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:175: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  state = self._get_connection(using).cluster.state(\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:434: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  return self._get_connection(using).indices.put_settings(\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/.venv/lib/python3.12/site-packages/elasticsearch_dsl/_sync/index.py:292: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  return self._get_connection(using).indices.open(index=self._name, **kwargs)\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/CountWords.py:26: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  for s in sc:\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/CountWords.py:28: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  tv = self.client.termvectors(index=index, id=s['_id'], fields=['text'])\n"
     ]
    }
   ],
   "source": [
    "for tokenizer_name in tokenizers:\n",
    "    for filter_list in filters:\n",
    "        my_analyzer = analyzer('default',\n",
    "            type='custom',\n",
    "            tokenizer=tokenizer(tokenizer_name),\n",
    "            filter=filter_list\n",
    "        )\n",
    "        \n",
    "        ind = Index(index, using=client);\n",
    "        ind.close()\n",
    "        ind.analyzer(my_analyzer);\n",
    "        ind.save()\n",
    "        ind.open()\n",
    "\n",
    "        # Contar las palabras usando la clase ElasticFunctionals\n",
    "        word_count = ef.count_words(index, alpha=False);\n",
    "\n",
    "        # Crear un DataFrame temporal con los resultados de la iteración actual\n",
    "        new_row = pd.DataFrame({\n",
    "            'tokenizer': [tokenizer_name],\n",
    "            'filters': [', '.join(filter_list)],\n",
    "            'word_count': [word_count]\n",
    "        })\n",
    "\n",
    "        # Concatenar la nueva fila al DataFrame existente\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tokenizer                  filters word_count\n",
      "0   whitespace                lowercase     167063\n",
      "1   whitespace  lowercase, asciifolding     167063\n",
      "2   whitespace   lowercase, porter_stem     149270\n",
      "3   whitespace         lowercase, kstem     153217\n",
      "4   whitespace      lowercase, snowball     141967\n",
      "5     standard                lowercase      61825\n",
      "6     standard  lowercase, asciifolding      61739\n",
      "7     standard   lowercase, porter_stem      41955\n",
      "8     standard         lowercase, kstem      45530\n",
      "9     standard      lowercase, snowball      39067\n",
      "10     classic                lowercase      59569\n",
      "11     classic  lowercase, asciifolding      59480\n",
      "12     classic   lowercase, porter_stem      39643\n",
      "13     classic         lowercase, kstem      43219\n",
      "14     classic      lowercase, snowball      36760\n",
      "15      letter                lowercase      54455\n",
      "16      letter  lowercase, asciifolding      54365\n",
      "17      letter   lowercase, porter_stem      34361\n",
      "18      letter         lowercase, kstem      38021\n",
      "19      letter      lowercase, snowball      33783\n"
     ]
    }
   ],
   "source": [
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que el tokenizer `whitespace` con el filtro `lowercase` produce el mayor conteo de palabras, alcanzando 167,063 tokens. Sin embargo, la aplicación de diferentes filtros de stemming, como `porter_stem` y `kstem`, reduce significativamente el número de tokens, con conteos de 149,270 y 153,217, respectivamente. Los tokenizers `standard` y `classic` generan conteos considerablemente más bajos, con el estándar alcanzando solo 61,825 palabras con el filtro `lowercase`. Esto muestra como el tokenizer y los filtros aplicados influyen de manera crítica en el procesamiento del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the most common word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/CountWords.py:65: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  for s in sc:\n",
      "/Users/nico/Desktop/UPC/CAIM/Lab/Lab2/CountWords.py:67: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  tv = self.client.termvectors(index=index, id=s['_id'], fields=['text'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Index Most Common Word   Count\n",
      "0      novels              the  206706\n",
      "1  scientific              the  257240\n"
     ]
    }
   ],
   "source": [
    "indices = ['novels', 'scientific']\n",
    "results_frequency = pd.DataFrame(columns=['Index', 'Most Common Word', 'Count'])\n",
    "\n",
    "ef = ElasticFunctionals(client)\n",
    "\n",
    "# Contar palabras más comunes en cada índice\n",
    "for index in indices:\n",
    "    word_counts = ef.count_word_frequency(index)\n",
    "    \n",
    "    if word_counts:\n",
    "        most_common_word = word_counts[0]  # Obtener la palabra más común\n",
    "        new_row = {\n",
    "            'Index': index,\n",
    "            'Most Common Word': most_common_word[0],\n",
    "            'Count': most_common_word[1]\n",
    "        }\n",
    "        results_frequency = pd.concat([results_frequency, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "print(results_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 3 Computing Tf-Idf and Cosine similarity\n",
    "\n",
    "Now is your turn to work in the session task.\n",
    "\n",
    "The idea is to program a script that given two document paths obtains their ids, computes the Tf-Idf representation of the documents and then computes and prints their cosine similarity\n",
    "\n",
    "**Follow the instructions** in the documentation and and **pay attention** to the documentation that you have to deliver for this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
